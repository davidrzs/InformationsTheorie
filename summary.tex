\documentclass[25pt]{sciposter}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsthm}

\usepackage[dvipsnames,usenames,svgnames,table]{xcolor} 
\usepackage{lipsum}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[german]{babel}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{gensymb}
\usepackage{tocloft}
\usepackage{empheq}

\usepackage{pgfplots}
\pgfplotsset{width=11cm,compat=1.9}


% for nice tableas
\usepackage{booktabs}

\graphicspath{ {img/} }

\geometry{
 landscape,
 a1paper,
 left=5mm,
 right=50mm,
 top=5mm,
 bottom=50mm,
 }
\usepackage{array}   % for \newcolumntype macro
\newcolumntype{L}{>{$}m{5.5cm}<{$}} % math-mode version of "l" column type

%BEGIN LISTINGDEF





\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
\newcommand{\limm}{\lim\limits_{n \to \infty}}
\newcommand{\limx}[1]{\lim\limits_{x \to #1}}
\newlength\dlf  % Define a new measure, dlf
\newcommand\alignedbox[2]{
% Argument #1 = before & if there were no box (lhs)
% Argument #2 = after & if there were no box (rhs)
&  % Alignment sign of the line
{
\settowidth\dlf{$\displaystyle #1$}
    % The width of \dlf is the width of the lhs, with a displaystyle font
\addtolength\dlf{\fboxsep+\fboxrule}
    % Add to it the distance to the box, and the width of the line of the box
\hspace{-\dlf}
    % Move everything dlf units to the left, so that & #1 #2 is aligned under #1 & #2
\boxed{#1 #2}
    % Put a box around lhs and rhs
}
}
\usepackage{graphicx,url}

%BEGIN TITLE
\title{\huge{Informationstheorie}}

\author{\large{David Zollikofer}}
%END TITLE


%\usepackage{eulervm}
\usepackage{mathpazo}
% begin custom commands
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\Nor}{\mathcal{N}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\E}{\operatorname{E}}
%\newcommand{\exp}{\operatorname{exp}}

\newcommand{\mc}{\mathcal}

% some shortcuts
\newcommand{\ds}{\displaystyle}
\newcommand{\arr}{\rightarrow}
\newcommand{\nop}[1]{}
\renewcommand{\hat}{\widehat}

% stuff for integrals
\newcommand{\intl}{\int\limits}
\newcommand{\rmd}{\mathrm{d}}


\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\usepackage[framemethod=latex]{mdframed}
\newenvironment{defn}[1]{\begin{mdframed}[backgroundcolor=blue!10,innertopmargin=15pt, nobreak=true,innerbottommargin=15pt]
		\textbf{#1 }
	}
	{ 
	\end{mdframed}
}

\newenvironment{important}{\begin{mdframed}[backgroundcolor=red!50,innertopmargin=15pt, innerbottommargin=15pt, nobreak=true]
		\Large
	}
	{ 
	\end{mdframed}
}

\newenvironment{lemma}{\begin{mdframed}[backgroundcolor=gray!50,innertopmargin=15pt, innerbottommargin=15pt, nobreak=true]
		\Large
	}
	{ 
	\end{mdframed}
}

\newenvironment{thm}[1]{\begin{mdframed}[nobreak=true,backgroundcolor=Emerald!10,innertopmargin=15pt, innerbottommargin=15pt]
		\textbf{#1 }
	}
	{ 
	\end{mdframed}
}



\newenvironment{trick}[1]{\begin{mdframed}[backgroundcolor=PineGreen!50,innertopmargin=15pt, innerbottommargin=15pt, nobreak=true]
			\textbf{#1 }
	}
	{ 
	\end{mdframed}
}


\usepackage{todonotes}
\newcommand{\TODO}[1]{\todo[inline]{\Large TODO:  #1}}





\DeclarePairedDelimiter\abs{\left|}{\right|}%



\setlength\abovedisplayskip{0pt}

\renewcommand{\familydefault}{\rmdefault}

% end custom commands

\begin{document}





\maketitle



\begin{multicols}{3}



\textit{Diese Zusammenfassung fasst die wichtigsten Definitionen und Theoreme der Informationstheorie Vorlesung gehalten von Dr. Luis Haug im Frühling 2020 zusammen. Alle Beweise finden sich entweder im Skript oder den Übungen und wurden hier weggelassen.}


\section{Entropie}


Die Entropie misst die Unsicherheit über den Wert eine Zufallsvariable $X$ bevor wir ihn erfahren.

\begin{defn}{Entropie einer Zufallsvariable}
Sei $X:\Omega \to \mathcal{X}$ mit Verteilung $P_X$ so dass $H(X) = H(P_X)$ so gilt

\begin{align*}
	H(X) := - \sum_x P_X(x) \log P_X(x)
\end{align*}
\end{defn}

\begin{defn}{Gemeinsame Entopie}
Die gemeinsame Entropie zweier Zufallsvariablen $(X,Y)$ mit $X:\Omega \to \mathcal{X}$ sowie $Y\to \mathcal{Y}$ mit gemeinsamer Verteilung $P_{X,Y}$ ist

\begin{align*}
	H(X,Y) = - \sum_{x,y} P_{X,Y} (x,y) \log P_{X,Y} (x,y)
\end{align*}
\end{defn}

\begin{defn}{Entropie bedingter Verteilung}
	Für $y\in Y(\Omega)$ mit $P_Y(y) > 0$ haben wir die auf $Y=y$ bedingte Verteilung $P_{X|Y=y}$ mit 
	\begin{align*}
		P_{X|Y=y} &= P(X=x | Y = y) = \frac{P_{X,Y}(x,y)}{P_Y(y)}
	\end{align*}
	
	für welche gilt
	
	\begin{align*}
	H(X|Y=y) = - \sum_{x} P_{X|Y=y} (x) \log P_{X|Y=y} (x)
	\end{align*}
	was die Unsicherheit über $X$ misst wenn $Y=y$ bekannt ist.
\end{defn}

\begin{defn}{Bedingte Entropie}
	Gegeben Zufallsvariablen $X,Y$ so definieren wir die bedingte Entropie von $X$ gegeben $Y$ als
	
	\begin{align*}
		H(X|Y) = \sum_y P_Y(y) H(X|Y=y)
	\end{align*}
	
	wobei $H(X|Y)$ der Erwartungswert von $H(X|Y=y)$ bezüglich aller möglichen $y$ ist.
\end{defn}

\begin{thm}{Kettenregel}
	Für jedes Paar an Zufallsvariablen $X,Y$ gilt
	\begin{align*}
		H(X|Y) + H(Y) = H(X,Y) = H(Y|X) + H(X)
	\end{align*}
\end{thm}
\begin{defn}{Wechselseitige Information}
	Wir definieren 
	
	\begin{align*}
		I(X;Y) = H(X) - H(X|Y)
	\end{align*}
	
	als die Reduktion der Unsicherheit über $X$ durch bekanntwerden von $Y$.
	
	Aufgrund der Kettenregel gilt
	
	\begin{align*}
		I(X;Y) &= H(X) + H(Y) - H(X,Y) = I(Y;X)
	\end{align*}
	

\end{defn}


	\begin{thm}{Satz zur wechselseitigen Information}
	Für alle $X,Y$ gilt $I(X;Y)\geq 0$ mit Gleichheit wenn $X,Y$ unabhängig sind.	
\end{thm}


\begin{defn}{Kullback-Leibler-Divergenz}
	Die Kullback-Leibler-Divergenz zwischen zwei Zufallsvariablen $P,Q$ ist definiert durch
	
	\begin{align*}
		D_{KL} (P||Q) = -\sum_{\omega}P(\omega) \log\left( \frac{Q(\omega)}{P(\omega)} \right) = \E \left[\log\frac{P}{Q}\right]
	\end{align*}
\end{defn}

\begin{thm}{Gibbs Ungleichung}
	Es gilt $D_{KL}(P||Q)\geq 0 $ mit Gleichheit genau dann wenn $P=Q$.
\end{thm}



\section{Quellcodierung}


\begin{defn}{Quellcode} Ein Quellcode über einem Alphabet $\mathcal{D}$ für eine Menge $\mathcal{X}$ ist eine Abbildung $C:\mathcal{X} \to \mathcal{D}^*$. Dabei ist $C(x)$ ein Codewort für $x\in \mathcal{X}$. Die Länge von $C(x)$ wird als $l_C(x)$ bezeichnet.
\end{defn}

\textbf{Ziel:} Wir möchte gerne die erwartete Länge eines Codes $C$

\begin{equation*}
L_C = \E[l_C] = \sum_{x\in X(\Omega)} P_X(x) l_C(x)
\end{equation*}
so klein wie möglich haben.




\begin{defn}{Eigenschaften von Codes}
	\begin{itemize}
		\item Ein Code $C$ heisst \textbf{nicht degeneriert}, falls $$x_i \neq x_j \implies C(x_i) \neq C(x_j) \quad \forall i\neq j$$
		\item Wir nennen $C$ \textbf{präfixfrei}, falls kein Codewort $C(x)$ ein Präfix eines anderen Codewortes ist.
	\end{itemize}
\end{defn}

Wir erweitern nun den Code $C$ zu einem Code $C^*$ für welchen $C^*(x_1 \ldots x_n) \mapsto C(x_1)\ldots C(x_n)$ gilt. Wenn $C^*$ nicht degeneriert ist, so nennen wir $C^*$ \textbf{eindeutig dekodierbar}.

\begin{thm}{Kraft Ungleichung}
\begin{itemize}
	\item Für jeden Präfixfreien Code $C:{X}(\Omega)\to \{0,1\}^*$ gilt 
	
	$$\sum_{x \in X(\Omega)} 2^{-l_C(x)} \leq 1$$
	
	\item Falls eine Funktion $l:X(\Omega)\to\N$ die Ungleichung
	
	$$\sum_{x\in X} 2^{-l(x)} \leq 1$$
	
	erfüllt, so existiert ein präfixfreier Code $C:X(\Omega) \to \{0,1\}^*$, so dass $l_C(x) = l(x)$.
\end{itemize}
\end{thm}

\begin{defn}{Optimaler Code}
	Wir nennen einen Code ${C'}:{X}(\Omega^) \to \{0,1\}^*$ optimal, falls für jeden anderen Code $C$ für $\mathcal{X}$ gilt $L_{C'} \leq L_C$. 
\end{defn}


\TODO{understand the proof of the following theorem}

\begin{thm}{Minimale Länge eines präfixfreien Codes}
Die erwartete Länge jedes präfixfreien Codes $C: X(\Omega) \to \{0,1\}^*$ erfüllt $L_C \geq H(X)$. Um $L_C = H(X)$ zu erreichen muss $p_i = 2^{-l_i}$ für alle $i$ mit $l_i \in \N$ sein.
\end{thm}

\begin{thm}{Existenz fast optimaler Codes}
		Jeder optimale präfixfreie Code $C':X(\Omega) \to \{0,1\}^*$ erfüllt 
		
		\begin{align*}
			H(X) \leq L_{C'} < H(X) + 1
		\end{align*}
		
		Diese Schranke erreichen wir indem wir indem wir $l_i := \lceil -\log p_i \rceil$ fordern.
		
		Wenn wir jeweils Blöcke der Länge $N$ Codieren so gilt 
		
		\begin{align*}
			H(X) \leq \frac{1}{N} L_{C_N} < H(X) + \frac{1}{N}
 		\end{align*}
womit insbesondere $\lim_{N\to \infty} L_{C_N} = H(X)$.

\end{thm}

 
\begin{thm}{Codes bezüglich falscher Verteilung}
		Wenn wir einen optimalen Code $C_q$ mit $l_{C_q}(x_i) = \lceil-\log q(x_i) \rceil$ verwenden um eine Eingabe mit der Verteilung $p$ zu Enkodieren so gilt
	
	\begin{align*}
		H(X) + D_{KL}(p||q) \leq \E_p[l_{C_q}] < H(X) + D_{KL}(p||q)  + 1
	\end{align*} 
	Wir bezahlen demnach also eine Strafe von $D_{KL}(p||q)$.
	
\end{thm}


\begin{defn}{Konstruktion von Huffman-Codes}
	Für eine Verteilung von $X$ gegeben durch $\vec{p} = (p_1,\ldots,p_n)$ konstruieren wir einen Huffman-Code rekursiv:
	\begin{itemize}
		\item Wähle die zwei kleinsten Wahrscheinlichkeiten $p_i$ und $p_j$.
		\item Wähle beide als Blätter und baue einen Baum aus drei Knoten wobei der Elternknoten die Wahrscheinlichkeit $p' = p_i + p_j$ hat. Gebe dem einen Kind $p_i$ den Präfix $1$, dem anderen $p_j$ den Präfix $1$.
		\item Entferne $p_i,p_j$ aus $\vec{p}$ und füge $p'$ zu $\vec{p}$ als neues Ereignis hinzu.
		\item Iteriere bis $\vec{p}$ nur noch ein Element hat mit $p_i = 1$, der Baum der zu dieser Wahrscheinlichkeit gehört ist der Huffman-Baum
	\end{itemize}
\end{defn}

\begin{thm}{Optimalität von Huffman-Codes}
Sei ${C_H}$ ein Huffman-Code für eine Zufallsvariable $X$ und sei $C^*$ ein optimaler präfixfreier Code für $X$, so ist $C_H$ ebenfalls ein optimaler Code für $X$ da gilt:
\begin{align*}
	L_{C_H} &= L_{C^*}
\end{align*}
\end{thm}

\begin{thm}{McMillan Ungleichung}
Jeder eindeutig dekodierbare Code $C:\mathcal{X}\to \{0,1\}^*$ erfüllt die Kraftungleichung
\begin{align*}
	\sum_{x\in\mathcal{X}} 2^{-l_C (x)} \leq 1
\end{align*}
\end{thm}

Daher folgt, dass es eigentlich reicht präfixfreie Codes anzuschauen, da allgemeine dekodierbare Codes die gleichen Eigenschaften haben.

%\vfill\null
%\columnbreak

\begin{defn}{Typische Folgen}
	Wir typische Ereignisse $\vec{x}$ auf $X^N$ Ereignisse die bei Erfolgswahrscheinlichkeit Bernoulli $p$ etwas mit Wahrscheinlichkeit $p^{pN} (1-p)^{(1-p)N}$ vorkommen.
	
	Für ein solche solches $\vec{x}$ gilt dann
	\begin{align*}
		-\frac{1}{N} \log P(\vec{x}) = - \frac{1}{N} \log \left( p^{pN} (1-p)^{(1-p)N}\right) = H(p,1-p)
	\end{align*} 
\end{defn}


\begin{defn}{Typische Menge}
	
	Für $\epsilon, N$ gegeben ist die typische Menge
	
	\begin{align*}
		T_{N,\epsilon} &= \bigg\{ \vec{x} \in \mathcal{X}^N |  \left| -\frac{1}{N}\log P(\vec{x}) - H(X) \right|  \leq \epsilon \bigg \}\\
		&= \bigg\{ \vec{x} \in \mathcal{X}^N | 2^{-N(H(X)+\epsilon)} < P(\vec{x}) < 2^{-N(H(X)-\epsilon)} \bigg \}
	\end{align*}
\end{defn}


\begin{thm}{Asymptotische Gleichverteilungseigenschaft}
	Falls die Zufallsvariablen $X,X_1,\ldots,x_n:\Omega\to \mathcal{X}$ i.i.d. sind, so gilt
	\begin{align*}
P\left( \left| -\frac{1}{N}\log P(X_1,\ldots,X_n) - H(X) \right|  > \epsilon \right) \to 0  \text{ für } N \to \infty
	\end{align*}
	
	Als Korollare folgen
	\begin{enumerate}
		\item $P(T_{N,\epsilon}) \to 1$ für $N \to \infty$
		\item $|T_{N,\epsilon}| < 2^{N(H(X) + \epsilon)}$
		\item $|T_{N,\epsilon}| > (1-\delta)2^{N(H(X)) - \epsilon}$ für jedes $\delta \in (0,1)$ und $N \gg 1$.
	\end{enumerate}
\end{thm}


\begin{defn}{Datenkompression via AEP}
Wir codieren mit Blockcode $C:\mathcal{X}^N \to \{0,1\}^*$ mit 
\begin{align*}
	C(x) \mapsto \begin{cases}
	"1" + \{0,1\}^{\lceil N(H(X) + \epsilon) \rceil} \ \text{ für } x \in T_{N,\epsilon}\\
	"0" + \{0,1\}^{N\log |\mathcal{X}|}
	\end{cases}
\end{align*}
für diesen Code gilt $L_C \leq N(H(X) + \epsilon')$ was pro Symbol eine Länge von $H(X)$ gibt bei grossem $N$.
\end{defn}



\begin{thm}{Shannon's Quellcodierungstheorem} Sei $H_\delta(X)$ die kleinste Anzahl an Bits um eine Menge $S_\delta$ mit $P(S_\delta)\geq 1-\delta$ zu beschreiben, dann gilt für alle $\epsilon > 0$ und $\delta\in (0,1)$ dass es ein $N_0 \in \N$ gibt, so dass für alle $N \geq N_0$
	
	\begin{align*}
\left|\frac{1}{N} H_{\delta} (X^N) - H(X) \right| < \epsilon
	\end{align*}
gilt. Dies hat zwei Konsequenzen:

\begin{itemize}
	\item Wir brauchen nur $H(X)$ Bits pro Symbol
	\item Auch wenn wir eine hohe Fehlerwahrscheinlichkeit in Kauf nehmen ($\delta$ gross), so benötigen wir immer noch $H(X)$ Bits für die Elemente in $S_\delta$.
\end{itemize}

\end{thm}



\begin{defn}{Lempel-Ziv-Codes}
	\textit{Enkodieren:}	Wir möchten kodieren und kennen die Auftretwahrscheinlichkeiten noch nicht. Dafür nehmen wir einen Eingabestring und bauen nach jedem Substring den wir noch nicht gesehen haben $|$ ein.
	
	\texttt{|1|0|11|01|010|00|10}
	
	Daraus bauen wir das Wörterbuch
	
{
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
	\hline
	Wort & $\lambda$ & \texttt{1} & \texttt{0}  & \texttt{11} & \texttt{01} & \texttt{010} & \texttt{00} & \texttt{10} \\
	\hline
	$s(n)$& 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
	\hline
	 $s_{bin}(n)$ & 000 & 001 & 010 & 011 & 100 & 101 & 110 & 111 \\
	\hline
	(ptr,bit) & & (,1) & (0,0) & (01,1) & (10,1) & (100,0) & (010,0) & (001,0)\\ 
	\hline
\end{tabular}}
Dabei habe wir den Pointer jeweils $\lceil \log_2 s(n) \rceil$ lang gemacht, sowie den ersten Pointer weggelassen.

Das Resultat wäre dann \texttt{100011101100001000010}


\textit{Dekodieren:} Wir nehmen den String und unterteilen ihn in jeweils $2^{k-2}$ Phrasen der Länge $k$, wobei wir das erste Bit separat betrachten.

Dies gibt \texttt{1|00|011|101|1000|0100|0010}
Hierbei ist das letzte Bit im Block das neue Bit und alle vorhergehenden sind Pointer.
\end{defn}

\textbf{Zusammenfassung}

Alle kennengelernten Kodierungsverfahren sind optimal!




\newpage

\end{multicols}
\end{document}
