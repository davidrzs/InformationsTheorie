\documentclass[25pt]{sciposter}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsthm}

\usepackage[dvipsnames,usenames,svgnames,table]{xcolor} 
\usepackage{lipsum}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[german]{babel}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{gensymb}
\usepackage{tocloft}
\usepackage{empheq}

\usepackage{pgfplots}
\pgfplotsset{width=11cm,compat=1.9}


% for nice tableas
\usepackage{booktabs}

\graphicspath{ {img/} }

\geometry{
 landscape,
 a1paper,
 left=5mm,
 right=50mm,
 top=5mm,
 bottom=50mm,
 }
\usepackage{array}   % for \newcolumntype macro
\newcolumntype{L}{>{$}m{5.5cm}<{$}} % math-mode version of "l" column type

%BEGIN LISTINGDEF





\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
\newcommand{\limm}{\lim\limits_{n \to \infty}}
\newcommand{\limx}[1]{\lim\limits_{x \to #1}}
\newlength\dlf  % Define a new measure, dlf
\newcommand\alignedbox[2]{
% Argument #1 = before & if there were no box (lhs)
% Argument #2 = after & if there were no box (rhs)
&  % Alignment sign of the line
{
\settowidth\dlf{$\displaystyle #1$}
    % The width of \dlf is the width of the lhs, with a displaystyle font
\addtolength\dlf{\fboxsep+\fboxrule}
    % Add to it the distance to the box, and the width of the line of the box
\hspace{-\dlf}
    % Move everything dlf units to the left, so that & #1 #2 is aligned under #1 & #2
\boxed{#1 #2}
    % Put a box around lhs and rhs
}
}
\usepackage{graphicx,url}

%BEGIN TITLE
\title{\huge{Informationstheorie}}

\author{\large{David Zollikofer}}
%END TITLE

\usepackage{palatino}
%\usepackage{eulervm}
\usepackage{mathpazo}
% begin custom commands
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\Nor}{\mathcal{N}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\E}{\operatorname{E}}
%\newcommand{\exp}{\operatorname{exp}}

\newcommand{\mc}{\mathcal}

% some shortcuts
\newcommand{\ds}{\displaystyle}
\newcommand{\arr}{\rightarrow}
\newcommand{\nop}[1]{}
\renewcommand{\hat}{\widehat}

% stuff for integrals
\newcommand{\intl}{\int\limits}
\newcommand{\rmd}{\mathrm{d}}


\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\usepackage[framemethod=TikZ]{mdframed}
\newenvironment{defn}[1]{\begin{mdframed}[backgroundcolor=blue!10,innertopmargin=15pt, innerbottommargin=15pt,nobreak=true]
		\textbf{#1 }
	}
	{ 
	\end{mdframed}
}

\newenvironment{important}{\begin{mdframed}[backgroundcolor=red!50,innertopmargin=15pt, innerbottommargin=15pt, nobreak=true]
		\Large
	}
	{ 
	\end{mdframed}
}

\newenvironment{lemma}{\begin{mdframed}[backgroundcolor=gray!50,innertopmargin=15pt, innerbottommargin=15pt, nobreak=true]
		\Large
	}
	{ 
	\end{mdframed}
}

\newenvironment{thm}[1]{\begin{mdframed}[backgroundcolor=Emerald!10,innertopmargin=15pt, innerbottommargin=15pt, nobreak=true]
		\textbf{#1 }
	}
	{ 
	\end{mdframed}
}



\newenvironment{trick}[1]{\begin{mdframed}[backgroundcolor=PineGreen!50,innertopmargin=15pt, innerbottommargin=15pt, nobreak=true]
			\textbf{#1 }
	}
	{ 
	\end{mdframed}
}


\usepackage{todonotes}
\newcommand{\TODO}[1]{\todo[inline]{\Large TODO:  #1}}





\DeclarePairedDelimiter\abs{\left|}{\right|}%



\setlength\abovedisplayskip{0pt}

\renewcommand{\familydefault}{\rmdefault}

% end custom commands

\begin{document}





\maketitle



\begin{multicols}{3}



\textit{Diese Zusammenfassung fasst die wichtigsten Definitionen und Theoreme der Informationstheorie Vorlesung gehalten von Dr. Luis Haug im Frühling 2020 zusammen. Alle Beweise finden sich entweder im Skript oder den Übungen und wurden hier weggelassen.}


\section{Entropie}


Die Entropie misst die Unsicherheit über den Wert eine Zufallsvariable $X$ bevor wir ihn erfahren.

\begin{defn}{Entropie einer Zufallsvariable}
Sei $X:\Omega \to \mathcal{X}$ mit Verteilung $P_X$ so dass $H(X) = H(P_X)$ so gilt

\begin{align*}
	H(X) := - \sum_x P_X(x) \log P_X(x)
\end{align*}
\end{defn}

\begin{defn}{Gemeinsame Entopie}
Die gemeinsame Entropie zweier Zufallsvariablen $(X,Y)$ mit $X:\Omega \to \mathcal{X}$ sowie $Y\to \mathcal{Y}$ mit gemeinsamer Verteilung $P_{X,Y}$ ist

\begin{align*}
	H(X,Y) = - \sum_{x,y} P_{X,Y} (x,y) \log P_{X,Y} (x,y)
\end{align*}
\end{defn}

\begin{defn}{Entropie bedingter Verteilung}
	Für $y\in Y(\Omega)$ mit $P_Y(y) > 0$ haben wir die auf $Y=y$ bedingte Verteilung $P_{X|Y=y}$ mit 
	\begin{align*}
		P_{X|Y=y} &= P(X=x | Y = y) = \frac{P_{X,Y}(x,y)}{P_Y(y)}
	\end{align*}
	
	für welche gilt
	
	\begin{align*}
	H(X|Y=y) = - \sum_{x} P_{X|Y=y} (x) \log P_{X|Y=y} (x)
	\end{align*}
	was die Unsicherheit über $X$ misst wenn $Y=y$ bekannt ist.
\end{defn}

\begin{defn}{Bedingte Entropie}
	Gegeben Zufallsvariablen $X,Y$ so definieren wir die bedingte Entropie von $X$ gegeben $Y$ als
	
	\begin{align*}
		H(X|Y) = \sum_y P_Y(y) H(X|Y=y)
	\end{align*}
	
	wobei $H(X|Y)$ der Erwartungswert von $H(X|Y=y)$ bezüglich aller möglichen $y$ ist.
\end{defn}

\begin{thm}{Kettenregel}
	Für jedes Paar an Zufallsvariablen $X,Y$ gilt
	\begin{align*}
		H(X|Y) + H(Y) = H(X,Y) = H(Y|X) + H(X)
	\end{align*}
\end{thm}
\begin{defn}{Wechselseitige Information}
	Wir definieren 
	
	\begin{align*}
		I(X;Y) = H(X) - H(X|Y)
	\end{align*}
	
	als die Reduktion der Unsicherheit über $X$ durch bekanntwerden von $Y$.
	
	Aufgrund der Kettenregel gilt
	
	\begin{align*}
		I(X;Y) &= H(X) + H(Y) - H(X,Y) = I(Y;X)
	\end{align*}
	

\end{defn}


	\begin{thm}{Satz zur wechselseitigen Information}
	Für alle $X,Y$ gilt $I(X;Y)\geq 0$ mit Gleichheit wenn $X,Y$ unabhängig sind.	
\end{thm}


\begin{defn}{Kullback-Leibler-Divergenz}
	Die Kullback-Leibler-Divergenz zwischen zwei Zufallsvariablen $P,Q$ ist definiert durch
	
	\begin{align*}
		D_{KL} (P||Q) = -\sum_{\omega}P(\omega) \log\left( \frac{Q(\omega)}{P(\omega)} \right) = \E \left[\log\frac{P}{Q}\right]
	\end{align*}
\end{defn}

\begin{thm}{Gibbs Ungleichung}
	Es gilt $D_{KL}(P||Q)\geq 0 $ mit Gleichheit genau dann wenn $P=Q$.
\end{thm}



\section{Quellcodierung}


\begin{defn}{Quellcode} Ein Quellcode über einem Alphabet $\mathcal{D}$ für eine Menge $\mathcal{X}$ ist eine Abbildung $C:\mathcal{X} \to \mathcal{D}^*$. Dabei ist $C(x)$ ein Codewort für $x\in \mathcal{X}$. Die Länge von $C(x)$ wird als $l_C(x)$ bezeichnet.
\end{defn}

\textbf{Ziel:} Wir möchte gerne die erwartete Länge eines Codes $C$

\begin{equation*}
L_C = \E[l_C] = \sum_{x\in X(\Omega)} P_X(x) l_C(x)
\end{equation*}
so klein wie möglich haben.




\begin{defn}{Eigenschaften von Codes}
	\begin{itemize}
		\item Ein Code $C$ heisst \textbf{nicht degeneriert}, falls $$x_i \neq x_j \implies C(x_i) \neq C(x_j) \quad \forall i\neq j$$
		\item Wir nennen $C$ \textbf{präfixfrei}, falls kein Codewort $C(x)$ ein Präfix eines anderen Codewortes ist.
	\end{itemize}
\end{defn}

Wir erweitern nun den Code $C$ zu einem Code $C^*$ für welchen $C^*(x_1 \ldots x_n) \mapsto C(x_1)\ldots C(x_n)$ gilt. Wenn $C^*$ nicht degeneriert ist, so nennen wir $C^*$ \textbf{eindeutig dekodierbar}.

\begin{thm}{Kraft Ungleichung}
\begin{itemize}
	\item Für jeden Präfixfreien Code $C:{X}(\Omega)\to \{0,1\}^*$ gilt 
	
	$$\sum_{x \in X(\Omega)} 2^{-l_C(x)} \leq 1$$
	
	\item Falls eine Funktion $l:X(\Omega)\to\N$ die Ungleichung
	
	$$\sum_{x\in X} 2^{-l(x)} \leq 1$$
	
	erfüllt, so existiert ein präfixfreier Code $C:X(\Omega) \to \{0,1\}^*$, so dass $l_C(x) = l(x)$.
\end{itemize}
\end{thm}

\begin{defn}{Optimaler Code}
	Wir nennen einen Code ${C'}:{X}(\Omega^) \to \{0,1\}^*$ optimal, falls für jeden anderen Code $C$ für $\mathcal{X}$ gilt $L_{C'} \leq L_C$. 
\end{defn}


\TODO{understand the proof of the following theorem}

\begin{thm}{Minimale Länge eines präfixfreien Codes}
Die erwartete Länge jedes präfixfreien Codes $C: X(\Omega) \to \{0,1\}^*$ erfüllt $L_C \geq H(X)$. Um $L_C = H(X)$ zu erreichen muss $p_i = 2^{-l_i}$ für alle $i$ mit $l_i \in \N$ sein.
\end{thm}

\begin{thm}{Existenz fast optimaler Codes}
		Jeder optimale präfixfreie Code $C':X(\Omega) \to \{0,1\}^*$ erfüllt 
		
		\begin{align*}
			H(X) \leq L_{C'} < H(X) + 1
		\end{align*}
		
		Diese Schranke erreichen wir indem wir indem wir $l_i := \lceil -\log p_i \rceil$ fordern.
		
		Wenn wir jeweils Blöcke der Länge $N$ Codieren so gilt 
		
		\begin{align*}
			H(X) \leq \frac{1}{N} L_{C_N} < H(X) + \frac{1}{N}
 		\end{align*}
womit insbesondere $\lim_{N\to \infty} L_{C_N} = H(X)$.

\end{thm}


\begin{thm}{Codes bezüglich falscher Verteilung}
	Wenn wir einen optimalen Code $C_q$ mit $l_{C_q}(x_i) = \lceil-\log q(x_i) \rceil$ verwenden um eine Eingabe mit der Verteilung $p$ zu Enkodieren so gilt
	
	\begin{align*}
		H(X) + D_{KL}(p||q) \leq \E_p[l_{C_q}] < H(X) + D_{KL}(p||q)  + 1
	\end{align*} 
	
	Wir bezahlen demnach also eine Strafe von $D_{KL}(p||q)$.
	
\end{thm}








\newpage

\end{multicols}
\end{document}
